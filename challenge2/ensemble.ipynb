{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AN2DL - Second Challenge\n",
    "\n",
    "Lorenzo Bardelli, Lorenzo Moretti, Luca Zani\n",
    "\n",
    "â `thegradientdescenders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Set debug for showing extra outputs\n",
    "DEBUG = False\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install transformers\n",
    "from transformers import AutoModel\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    torch.mps.manual_seed(SEED)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure plot display settings\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation and normalization for datasets\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "class AugmentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels, augmentation=None, normalize: str = None):\n",
    "        # Convert to tensor and permute to (N, C, H, W)\n",
    "        self.data = torch.from_numpy(data).permute(0, 3, 1, 2)\n",
    "        self.labels = torch.from_numpy(labels).long()\n",
    "        self.augmentation = augmentation\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # ImageNet normalization transform\n",
    "        if normalize == \"imagenet\":\n",
    "            self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        else:\n",
    "            self.normalize = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx].clone()  # Clone to avoid modifying original\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.augmentation:\n",
    "            image = self.augmentation(image)\n",
    "        \n",
    "        if self.normalize:\n",
    "            image = self.normalize(image)  # Normalize only RGB channels\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhikonTransferLearning(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.3, freeze_backbone=True, unfreeze_last_n_layers=0):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(\"owkin/phikon\")\n",
    "        self.hidden_size = self.backbone.config.hidden_size\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            # Unfreeze LayerNorm for domain adaptation\n",
    "            for module in self.backbone.modules():\n",
    "                if isinstance(module, nn.LayerNorm):\n",
    "                    for param in module.parameters():\n",
    "                        param.requires_grad = True\n",
    "\n",
    "            # Unfreeze last N transformer layers for fine-tuning \n",
    "            if unfreeze_last_n_layers > 0:\n",
    "                total_layers = len(self.backbone.encoder.layer)\n",
    "                layers_to_unfreeze = self.backbone.encoder.layer[-unfreeze_last_n_layers:]\n",
    "                for layer in layers_to_unfreeze:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = True\n",
    "                print(f\"Unfroze last {unfreeze_last_n_layers} transformer layer(s) out of {total_layers}\")\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 512),\n",
    "            nn.LayerNorm(512), \n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate * 0.7),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate * 0.5),\n",
    "\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "         # Resize to 224x224 if needed (Phikon requires this size)\n",
    "        if x.shape[-1] != 224 or x.shape[-2] != 224:\n",
    "            x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "        outputs = self.backbone(x)\n",
    "        cls_token = outputs.last_hidden_state[:, 0] \n",
    "        return self.classifier(cls_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetB0TransferLearning(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.3, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.efficientnet_b0(\n",
    "            weights=torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.features.parameters():\n",
    "                param.requires_grad = False\n",
    "            # Unfreeze GroupNorm/BatchNorm for domain adaptation\n",
    "            for module in self.backbone.features.modules():\n",
    "                if isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                    for param in module.parameters():\n",
    "                        param.requires_grad = True\n",
    "\n",
    "        # Replace classifier with more regularization\n",
    "        in_features = self.backbone.classifier[-1].in_features\n",
    "        # NOTE: MPS doesn't support inplace\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate, inplace=False),\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(p=dropout_rate * 0.5, inplace=False),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Load Data and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories and paths\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "base_dir = os.path.dirname(os.path.abspath(\"__file__\")) if \"__file__\" in globals() else os.getcwd()\n",
    "current_dir = os.path.join(base_dir, \"preprocessed\")\n",
    "dataset_dir = os.path.join(base_dir, \"dataset\")\n",
    "\n",
    "print(f\"Data directory: {current_dir}\")\n",
    "print(f\"Dataset directory: {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load patches from folder\n",
    "\n",
    "def load_patches_from_folder(folder, max_workers=None):\n",
    "    \"\"\"Load patches grouped by image ID.\n",
    "    \n",
    "    For files named img_XXXX_patch_YY.png, groups patches by image ID (XXXX).\n",
    "    Returns list of lists: [[patches for img 0], [patches for img 1], ...]\n",
    "    \"\"\"\n",
    "    filenames = sorted(glob.glob(os.path.join(folder, 'img_*.png')), \n",
    "                       key=lambda x: (int(os.path.basename(x).split('_')[1]), \n",
    "                                      int(os.path.basename(x).split('_')[3].split('.')[0])))\n",
    "    \n",
    "    def load_image(file):\n",
    "        img = cv2.imread(file)\n",
    "        return img.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel image loading\n",
    "    if max_workers is None:\n",
    "        max_workers = min(8, (os.cpu_count() or 4))\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        all_patches = list(executor.map(load_image, filenames))\n",
    "    \n",
    "    # Group patches by image ID\n",
    "    image_patches = defaultdict(list)\n",
    "    for filepath, patch in zip(filenames, all_patches):\n",
    "        img_id = int(os.path.basename(filepath).split('_')[1])\n",
    "        image_patches[img_id].append(patch)\n",
    "    \n",
    "    # Convert to sorted list of lists\n",
    "    sorted_ids = sorted(image_patches.keys())\n",
    "    patches = [image_patches[img_id] for img_id in sorted_ids]\n",
    "    \n",
    "    return patches, sorted_ids\n",
    "\n",
    "# Load test data\n",
    "test_patches, test_imageIndeces = load_patches_from_folder(current_dir + '/test_data')\n",
    "\n",
    "print(f\"Loaded {len(test_patches)} test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants and label mapping\n",
    "BATCH_SIZE = 32\n",
    "num_classes = 4\n",
    "\n",
    "label_mapping = {\n",
    "    'Luminal A': 0,\n",
    "    'Luminal B': 1,\n",
    "    'HER2(+)': 2,\n",
    "    'Triple negative': 3\n",
    "}\n",
    "\n",
    "print(f\"Label mapping: {label_mapping}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance and load pre-trained weights\n",
    "b0_model = EfficientNetB0TransferLearning(\n",
    "    num_classes=num_classes, \n",
    "    dropout_rate=0.65, \n",
    "    freeze_backbone=True,\n",
    ").to(device)\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model_path = 'ensemble_models/efficientnetb0_tl_frozen_model.pt'\n",
    "b0_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "b0_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model loaded from: {model_path}\")\n",
    "print(f\"Model set to evaluation mode\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in b0_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in b0_model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "phikon_model = PhikonTransferLearning(\n",
    "    num_classes=num_classes, \n",
    "    dropout_rate=0.5, \n",
    "    freeze_backbone=True,\n",
    "    unfreeze_last_n_layers=0,\n",
    ").to(device)\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model_path = 'ensemble_models/phikon_tl_frozen_model.pt'\n",
    "phikon_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "phikon_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model loaded from: {model_path}\")\n",
    "print(f\"Model set to evaluation mode\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in phikon_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in phikon_model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict function for ensemble models\n",
    "def predict_ensemble(models, normalize, test_patches, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Predict classes for test patches using ensemble of models.\n",
    "    \n",
    "    Args:\n",
    "        models: list of trained PyTorch models\n",
    "        normalize: array of normalization method for dataset\n",
    "        test_patches: list of lists of patches for each test image\n",
    "        batch_size: batch size for DataLoader\n",
    "    Returns:\n",
    "        predictions: numpy array of predicted class indices for each test image\n",
    "    \"\"\"\n",
    "\n",
    "    all_model_preds = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model_preds = []\n",
    "        for patches in test_patches:\n",
    "            dataset = AugmentDataset(\n",
    "                data=np.array(patches), \n",
    "                labels=np.zeros(len(patches)),  # Dummy labels\n",
    "                augmentation=None,\n",
    "                normalize=normalize[i]\n",
    "            )\n",
    "            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            patch_preds = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, _ in dataloader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    probs = F.softmax(outputs, dim=1)\n",
    "                    patch_preds.append(probs.cpu().numpy())\n",
    "            \n",
    "            patch_preds = np.vstack(patch_preds)\n",
    "            avg_patch_pred = np.mean(patch_preds, axis=0)\n",
    "            model_preds.append(avg_patch_pred)\n",
    "        \n",
    "        all_model_preds.append(np.vstack(model_preds))\n",
    "    \n",
    "    # Average predictions from all models\n",
    "    final_preds = np.mean(np.array(all_model_preds), axis=0)\n",
    "    predicted_classes = np.argmax(final_preds, axis=1)\n",
    "\n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_preds_tta = predict_ensemble([phikon_model, b0_model], normalize=[False, \"imagenet\"], test_patches=test_patches)\n",
    "\n",
    "# Create inverse mapping from class index to label name\n",
    "inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "submission_df_tta = pd.DataFrame({\n",
    "    'sample_index': [f'img_{test_imageIndeces[i]:04d}.png' for i in range(len(final_test_preds_tta))],\n",
    "    'label': [inverse_label_mapping[pred] for pred in final_test_preds_tta]\n",
    "})\n",
    "\n",
    "submission_df_tta.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\\\nTTA submission file created: submission.csv\")\n",
    "print(f\"Prediction distribution with TTA:\")\n",
    "print(submission_df_tta['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Test time augmentation\n",
    "Test time augmentation (TTA) is a technique used in machine learning, particularly in computer vision, to improve the performance of models during inference. The idea is to apply various transformations to the input data at test time, generate multiple predictions for each transformed input, and then aggregate these predictions to obtain a final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_tta(model, test_patches, num_tta=5, normalize=True):\n",
    "    \"\"\"\n",
    "    Generate predictions using Test-Time Augmentation.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_patches: List of patch arrays per image\n",
    "        num_tta: Number of augmented predictions to average (default: 5)\n",
    "        use_normalization: Whether to apply ImageNet normalization (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Array of predicted class indices\n",
    "    \"\"\"\n",
    "    # TTA augmentations (mild transforms that preserve label)\n",
    "    if normalize:\n",
    "        tta_transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        ])\n",
    "    else:\n",
    "        tta_transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "        ])\n",
    "    \n",
    "    # Prepare test patches\n",
    "    X_test_list = []\n",
    "    image_idx_test_list = []\n",
    "    for i, patches in enumerate(test_patches):\n",
    "        for patch in patches:\n",
    "            X_test_list.append(patch)\n",
    "            image_idx_test_list.append(i)\n",
    "    \n",
    "    X_test_final = np.array(X_test_list)\n",
    "    image_idx_test_final = np.array(image_idx_test_list)\n",
    "    \n",
    "    all_probs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for tta_iter in range(num_tta):\n",
    "            # Create dataset with TTA transforms\n",
    "            dummy_labels = np.zeros(len(X_test_final), dtype=np.int64)\n",
    "            \n",
    "            if tta_iter == 0:\n",
    "                # First iteration: no augmentation (original)\n",
    "                if normalize:\n",
    "                    test_ds = AugmentDataset(X_test_final, dummy_labels, augmentation=None, normalize=\"imagenet\")\n",
    "                else:\n",
    "                    test_ds = AugmentDataset(X_test_final, dummy_labels, augmentation=None, normalize=None)\n",
    "            else:\n",
    "                # Subsequent iterations: apply TTA\n",
    "                test_ds = AugmentDataset(X_test_final, dummy_labels, augmentation=tta_transforms, normalize=None)\n",
    "            \n",
    "            test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "            \n",
    "            # Collect predictions for this TTA iteration\n",
    "            tta_probs = []\n",
    "            for xb, _ in test_loader:\n",
    "                xb = xb.to(device)\n",
    "                logits = model(xb)\n",
    "                probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "                tta_probs.append(probs)\n",
    "            \n",
    "            all_probs.append(np.concatenate(tta_probs))\n",
    "            print(f\"TTA iteration {tta_iter + 1}/{num_tta} completed\")\n",
    "    \n",
    "    # Average probabilities across all TTA iterations\n",
    "    avg_tta_probs = np.mean(all_probs, axis=0)\n",
    "    \n",
    "    # Aggregate by image (average across patches)\n",
    "    image_probs = defaultdict(list)\n",
    "    for i, (probs, img_i) in enumerate(zip(avg_tta_probs, image_idx_test_final)):\n",
    "        image_probs[img_i].append(probs)\n",
    "    \n",
    "    final_preds = []\n",
    "    for img_i in sorted(image_probs.keys()):\n",
    "        avg_probs = np.mean(image_probs[img_i], axis=0)\n",
    "        final_preds.append(avg_probs.argmax())\n",
    "    \n",
    "    return np.array(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with TTA (set num_tta=1 to disable TTA)\n",
    "USE_TTA = True\n",
    "NUM_TTA = 5 if USE_TTA else 1\n",
    "\n",
    "print(f\"Generating predictions with TTA (num_tta={NUM_TTA})...\")\n",
    "final_test_preds_tta = predict_with_tta(phikon_model, test_patches, num_tta=NUM_TTA, normalize=False)\n",
    "\n",
    "# Create inverse mapping from class index to label name\n",
    "inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "submission_df_tta = pd.DataFrame({\n",
    "    'sample_index': [f'img_{test_imageIndeces[i]:04d}.png' for i in range(len(final_test_preds_tta))],\n",
    "    'label': [inverse_label_mapping[pred] for pred in final_test_preds_tta]\n",
    "})\n",
    "\n",
    "submission_df_tta.to_csv('submission_tta.csv', index=False)\n",
    "\n",
    "print(f\"\\\\nTTA submission file created: submission_tta.csv\")\n",
    "print(f\"Prediction distribution with TTA:\")\n",
    "print(submission_df_tta['label'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
