{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f98045",
   "metadata": {},
   "source": [
    "# AN2DL - Second Challenge\n",
    "\n",
    "Lorenzo Bardelli, Lorenzo Moretti, Luca Zani\n",
    "\n",
    "â `thegradientdescenders`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Clean the dataset from unwanted samples of shrek and green blobs and augment it by creating \"patches\", each image in the dataset will be used to extract patches; those will allow us to have more training data to work with\n",
    "\n",
    "Where in the image to extract a patch is determined by the mask attached to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import psutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\")) if \"__file__\" in globals() else os.getcwd()\n",
    "dataset_dir = os.path.join(current_dir, \"dataset\")\n",
    "print(f\"Dataset directory: {dataset_dir}\")\n",
    "\n",
    "os.makedirs(os.path.join(current_dir, \"preprocessed\"), exist_ok=True)\n",
    "SAVE_DIR = os.path.join(current_dir, 'preprocessed')\n",
    "\n",
    "os.makedirs(os.path.join(SAVE_DIR, \"train_data\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(SAVE_DIR, \"test_data\"), exist_ok=True)\n",
    "\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded bad indices, detected via a python script but also added manually in some cases\n",
    "BAD_INDICES = {\n",
    "    1,\n",
    "    5,\n",
    "    8,\n",
    "    12,\n",
    "    18,\n",
    "    20,\n",
    "    22,\n",
    "    27,\n",
    "    28,\n",
    "    36,\n",
    "    44,\n",
    "    47,\n",
    "    48,\n",
    "    52,\n",
    "    62,\n",
    "    78,\n",
    "    85,\n",
    "    90,\n",
    "    94,\n",
    "    95,\n",
    "    129,\n",
    "    130,\n",
    "    133,\n",
    "    136,\n",
    "    138,\n",
    "    148,\n",
    "    150,\n",
    "    155,\n",
    "    159,\n",
    "    161,\n",
    "    175,\n",
    "    178,\n",
    "    179,\n",
    "    180,\n",
    "    184,\n",
    "    187,\n",
    "    189,\n",
    "    193,\n",
    "    196,\n",
    "    222,\n",
    "    251,\n",
    "    254,\n",
    "    263,\n",
    "    268,\n",
    "    286,\n",
    "    293,\n",
    "    313,\n",
    "    319,\n",
    "    333,\n",
    "    342,\n",
    "    344,\n",
    "    346,\n",
    "    355,\n",
    "    368,\n",
    "    371,\n",
    "    376,\n",
    "    380,\n",
    "    390,\n",
    "    393,\n",
    "    407,\n",
    "    410,\n",
    "    415,\n",
    "    424,\n",
    "    443,\n",
    "    453,\n",
    "    459,\n",
    "    463,\n",
    "    486,\n",
    "    497,\n",
    "    498,\n",
    "    499,\n",
    "    509,\n",
    "    521,\n",
    "    530,\n",
    "    531,\n",
    "    533,\n",
    "    537,\n",
    "    540,\n",
    "    544,\n",
    "    547,\n",
    "    557,\n",
    "    558,\n",
    "    560,\n",
    "    565,\n",
    "    567,\n",
    "    572,\n",
    "    578,\n",
    "    580,\n",
    "    586,\n",
    "    602,\n",
    "    603,\n",
    "    607,\n",
    "    609,\n",
    "    614,\n",
    "    620,\n",
    "    623,\n",
    "    629,\n",
    "    635,\n",
    "    639,\n",
    "    643,\n",
    "    644,\n",
    "    645,\n",
    "    646,\n",
    "    656,\n",
    "    657,\n",
    "    658,\n",
    "    670,\n",
    "    673,\n",
    "    675,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patches(id, patches, data_type='train'):\n",
    "    \"\"\"Save patches to appropriate subfolder.\n",
    "    \n",
    "    Args:\n",
    "        id: Image ID\n",
    "        patches: List of patch arrays\n",
    "        data_type: 'train' or 'test' to specify which subfolder to use\n",
    "    \"\"\"\n",
    "    if SAVE_DIR:\n",
    "        save_path = os.path.join(SAVE_DIR, f\"{data_type}_data\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        for count, img_patch in enumerate(patches):\n",
    "            # Convert float32 (0-1) to uint8 (0-255) for saving\n",
    "            img_patch_uint8 = (img_patch * 255).astype(np.uint8)\n",
    "            # Format: img_XXXX_patch_YY.png where XXXX is image ID (4 digits), YY is patch number (2 digits)\n",
    "            cv2.imwrite(os.path.join(save_path, f\"img_{id:04d}_patch_{count:02d}.png\"), img_patch_uint8)\n",
    "\n",
    "# Function to load images and masks from a folder with parallel loading\n",
    "def load_patches_from_folder(folder, use_parallel=True, max_workers=8, patch_size=128, stride=64, max_patches=12, data_type='train', filter_bad_indices=True):\n",
    "    \"\"\"Load and preprocess images and masks from a specified folder with parallel loading.\n",
    "    \n",
    "    Args:\n",
    "        folder: Path to the folder containing images\n",
    "        use_parallel: Whether to use parallel processing\n",
    "        max_workers: Number of parallel workers\n",
    "        patch_size: Size of patches to extract\n",
    "        stride: Stride for patch extraction\n",
    "        max_patches: Maximum number of patches per image\n",
    "        data_type: 'train' or 'test' - determines subfolder for saving and whether to filter bad indices\n",
    "        filter_bad_indices: If True, skip images in blacklist (only applies to 'train' data)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (patches, indices) where patches is a list of np.arrays and indices is a list of image indices\n",
    "    \"\"\"\n",
    "\n",
    "    def make_patches(image, mask, img_id=None):\n",
    "        \"\"\"Extracts patches from image and mask, keeping those with significant mask content.\"\"\"\n",
    "        \n",
    "        H, W = mask.shape[:2]\n",
    "\n",
    "        top_patches = []\n",
    "\n",
    "        count = 0\n",
    "        for y in range(0, H - patch_size + 1, stride):\n",
    "            for x in range(0, W - patch_size + 1, stride):\n",
    "                img_patch = image[y:y+patch_size, x:x+patch_size]\n",
    "                mask_patch = mask[y:y+patch_size, x:x+patch_size]\n",
    "\n",
    "                # Check importance\n",
    "                mask_ratio = np.count_nonzero(mask_patch) / (patch_size * patch_size)\n",
    "                if mask_ratio > 0:\n",
    "                    top_patches.append( (img_patch, mask_ratio) )\n",
    "                    count += 1\n",
    "\n",
    "        return [x[0] for x in sorted(top_patches, key=lambda x: x[1], reverse=True)[:max_patches]]\n",
    "    \n",
    "    def print_memory():\n",
    "        print(f\"RAM used: {psutil.Process().memory_info().rss / 1024 / 1024:.0f} MB\")\n",
    "    \n",
    "    def process_single_image(filename):\n",
    "        \"\"\"Process a single image-mask pair. Returns (idx, patches) or None.\"\"\"\n",
    "        # Extract index from filename (e.g., img_0000.png -> 0)\n",
    "        idx = int(filename.split('_')[1].split('.')[0])\n",
    "        \n",
    "        # Only filter bad indices for training data\n",
    "        if filter_bad_indices and data_type == 'train' and idx in BAD_INDICES:\n",
    "            return None\n",
    "        \n",
    "        # Load image and mask\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        mask = cv2.imread(os.path.join(folder, filename.replace('img_', 'mask_')), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if img is None or mask is None:\n",
    "            return None\n",
    "\n",
    "        # Convert to float32 and normalize BEFORE padding (for filtering)\n",
    "        img_original = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Convert mask to binary (single channel): 0 or 1\n",
    "        mask_binary = (mask > 127).astype(np.float32)  # Set threshold at 127, value > 127 -> 1, else 0\n",
    "        mask_binary = np.expand_dims(mask_binary, axis=-1)[:,:,0]  # Add channel dimension (H, W, 1)\n",
    "\n",
    "        patches = make_patches(img_original, mask_binary, img_id=idx)\n",
    "\n",
    "        # Save patches to appropriate subfolder based on data type\n",
    "        save_patches(idx, patches, data_type=data_type)\n",
    "        return (idx, patches)\n",
    "    \n",
    "    # Get sorted list of image files (not masks) - using glob is faster\n",
    "    filenames = sorted(glob.glob(os.path.join(folder, 'img_*.png')), key=lambda x: int(os.path.basename(x).split('_')[1].split('.')[0]))\n",
    "    filenames = [os.path.basename(f) for f in filenames]\n",
    "    \n",
    "    if not filenames:\n",
    "        raise ValueError(f\"No image files found in {folder}\")\n",
    "    \n",
    "    print(f\"Loading {len(filenames)} images from {folder}...\")\n",
    "    \n",
    "    print_memory()\n",
    "\n",
    "    if use_parallel and len(filenames) > 10:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            results = list(executor.map(process_single_image, filenames))\n",
    "    else:\n",
    "        results = [process_single_image(f) for f in filenames]\n",
    "    \n",
    "    # Filter out None results and unpack (idx, patches) tuples\n",
    "    # Results from executor.map are in the same order as input filenames\n",
    "    valid_results = [r for r in results if r is not None]\n",
    "    indices = [r[0] for r in valid_results]\n",
    "    patches = [np.array(r[1], dtype=np.float32) for r in valid_results]\n",
    "    return patches, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data (with blacklist filtering)\n",
    "print(\"\\n=== Processing Training Data ===\")\n",
    "train_patches, imageIndeces = load_patches_from_folder(\n",
    "    dataset_dir + '/' + 'train_data', \n",
    "    patch_size=128, \n",
    "    data_type='train',\n",
    "    filter_bad_indices=True\n",
    ")\n",
    "\n",
    "# Load test data (without blacklist filtering)\n",
    "print(\"\\n=== Processing Test Data ===\")\n",
    "test_patches, test_imageIndeces = load_patches_from_folder(\n",
    "    dataset_dir + '/' + 'test_data', \n",
    "    patch_size=128, \n",
    "    data_type='test',\n",
    "    filter_bad_indices=False\n",
    ")\n",
    "\n",
    "# Load training labels\n",
    "labels_df = pd.read_csv(os.path.join(dataset_dir, 'train_labels.csv'))\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Loaded {len(train_patches)} training images\")\n",
    "print(f\"Training image indices: {imageIndeces[:10]} ...\")\n",
    "print(f\"Loaded {len(test_patches)} test images\")\n",
    "print(f\"Test image indices: {test_imageIndeces[:10]} ...\")\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(labels_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original images, masks, and extracted patches\n",
    "\n",
    "def display_images(num_images=10, max_patches_display=6):\n",
    "    \"\"\"Display original images with mask overlay and their extracted patches.\"\"\"\n",
    "    \n",
    "    folder = os.path.join(dataset_dir, 'train_data')\n",
    "    \n",
    "    fig, axes = plt.subplots(num_images, max_patches_display + 2, figsize=(20, 3 * num_images))\n",
    "    \n",
    "    for i in range(min(num_images, len(train_patches))):\n",
    "        idx = random.randint(0, len(train_patches) - 1)\n",
    "        actual_img_id = imageIndeces[idx]  # Get the actual image ID\n",
    "\n",
    "        # Use zero-padded filename format (img_0000.png, img_0001.png, etc.)\n",
    "        img_path = os.path.join(folder, f'img_{actual_img_id:04d}.png')\n",
    "        mask_path = os.path.join(folder, f'mask_{actual_img_id:04d}.png')\n",
    "        \n",
    "        # Load original image\n",
    "        img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        \n",
    "        # Load mask\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask_binary = (mask > 127).astype(np.float32)\n",
    "        \n",
    "        # Show original image\n",
    "        axes[i, 0].imshow(img_rgb)\n",
    "        axes[i, 0].set_title(f\"Original img_{actual_img_id:04d}\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Show image with mask overlay (red tint on masked regions)\n",
    "        overlay = img_rgb.copy()\n",
    "        overlay[mask_binary > 0.5] = overlay[mask_binary > 0.5] * 0.5 + np.array([1.0, 0.0, 0.0]) * 0.5\n",
    "        axes[i, 1].imshow(np.clip(overlay, 0, 1))\n",
    "        axes[i, 1].set_title(f\"Mask overlay\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Show patches\n",
    "        patches = train_patches[idx]\n",
    "        for j in range(max_patches_display):\n",
    "            ax = axes[i, j + 2]\n",
    "            if j < len(patches):\n",
    "                patch_rgb = cv2.cvtColor((patches[j] * 255).astype(np.uint8), cv2.COLOR_BGR2RGB) / 255.0\n",
    "                ax.imshow(patch_rgb)\n",
    "                ax.set_title(f\"Patch {j}\")\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy train_labels.csv from dataset to preprocessed folder\n",
    "import shutil\n",
    "\n",
    "src_labels_path = os.path.join(dataset_dir, 'train_labels.csv')\n",
    "dst_labels_path = os.path.join(SAVE_DIR, 'train_labels.csv')\n",
    "\n",
    "shutil.copy(src_labels_path, dst_labels_path)\n",
    "\n",
    "print(f\"Copied {src_labels_path} -> {dst_labels_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
